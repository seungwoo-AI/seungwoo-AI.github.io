# Data Science | Deep Critical Exploration

This document presents personal reflections and critical investigations developed during the study of Data Science.  
It highlights deep questions, conceptual explorations, mathematical formulations, and key visual intuitions.

---

## 1. Structure of Vector Spaces

- **Initial Question:**  
  What exactly distinguishes a span from a basis?  
  How does dimension emerge from these structures?

- **Exploration:**  
  Span: all linear combinations of given vectors.  
  Basis: a minimal, independent spanning set.  
  Dimension is defined by the size of the basis.

- **Visualization:**  
  Visualized span as a plane generated by two vectors; basis ensures no redundancy.

- **Key Formula:**  
  > $$\text{Span}\{v_1, v_2, ..., v_k\} = \{ \sum_{i=1}^{k} a_i v_i \mid a_i \in \mathbb{R} \}$$

- **Summary Insight:**  
  > A basis is a minimal spanning set; dimension is the number of basis vectors.

---

## 2. Deep Interpretation of Linear Transformations

- **Initial Question:**  
  How do the kernel and image describe the 'loss' and 'preservation' of information?

- **Exploration:**  
  Kernel maps vectors to zero (information loss), image captures output space.

- **Visualization:**  
  Transforming a plane into a line: dimensions collapse (nontrivial kernel).

- **Key Formula:**  
  > $$\text{dim}(\text{ker}(T)) + \text{dim}(\text{im}(T)) = \text{dim}(V)$$

- **Summary Insight:**  
  > Kernel shows what collapses; image shows what survives.

---

## 3. Composition of Linear Transformations

- **Initial Question:**  
  What happens structurally when composing multiple transformations?

- **Exploration:**  
  Composition can amplify or reduce dimensions depending on each map's properties.

- **Visualization:**  
  Composition of two projections: may collapse space further.

- **Key Formula:**  
  > $$S \circ T(v) = S(T(v))$$

- **Summary Insight:**  
  > Composition intertwines effects; dimension behavior depends on each map's structure.

---

## 4. Nature of Isomorphisms

- **Initial Question:**  
  What does it truly mean for two vector spaces to be "the same"?

- **Exploration:**  
  Isomorphism requires bijection: injectivity (nullity = 0) and surjectivity (full image).

- **Visualization:**  
  One-to-one and onto mapping between spaces (no collapse or gaps).

- **Key Formula:**  
  > $$T \text{ is isomorphism} \iff T \text{ linear, injective, surjective}$$

- **Summary Insight:**  
  > Isomorphic spaces are structurally identical despite different representations.

---

## 5. Diagonalizability and Structural Understanding

- **Initial Question:**  
  Why can't all matrices be diagonalized?

- **Exploration:**  
  Full set of independent eigenvectors is required; defective matrices lack this.

- **Visualization:**  
  Stretching along axes (diagonalizable) vs entangled shearing (non-diagonalizable).

- **Key Formula:**  
  > Diagonalization:  
  > $$A = PDP^{-1}$$  
  where \( D \) is diagonal.

- **Summary Insight:**  
  > Diagonalizability requires enough independent eigenvectors to span the space.

---

## 6. Rank, Nullity, and Geometric Intuition

- **Initial Question:**  
  How does Rank-Nullity reflect transformation geometry?

- **Exploration:**  
  Rank measures preserved dimensions; nullity measures collapsed dimensions.

- **Visualization:**  
  Plane compressed to a line: nullity = 1, rank = 1.

- **Key Formula:**  
  > $$\text{dim}(V) = \text{rank}(T) + \text{nullity}(T)$$

- **Summary Insight:**  
  > Rank and nullity describe how input dimensions split under transformation.

---

## 7. PLU Decomposition and Determinant Structure

- **Initial Question:**  
  How does PLU decomposition simplify determinant computation?

- **Exploration:**  
  LU decomposition + permutation matrix allows determinant via triangular matrix.

- **Visualization:**  
  Matrix broken into simple triangular steps.

- **Key Formula:**  
  > $$\det(A) = \det(P) \times \det(L) \times \det(U)$$  
  and  
  > $$\det(L) = \det(U) = \text{product of diagonals}$$

- **Summary Insight:**  
  > PLU decomposition transforms determinant computation into simple multiplications.

---

## 8. Deeper View on Eigendecomposition

- **Initial Question:**  
  What insights does eigendecomposition provide into transformations?

- **Exploration:**  
  Reframes transformation as scaling along invariant directions (eigenvectors).

- **Visualization:**  
  Stretching/compressing along specific axes.

- **Key Formula:**  
  > $$Av = \lambda v$$  
  (for eigenvector \(v\) and eigenvalue \(\lambda\)).

- **Summary Insight:**  
  > Eigendecomposition reveals the simplest "natural frame" for a transformation.

---

# Summary

Through structured questioning, mathematical formalization, and conceptual visualization,  
I developed deeper intuition about vector spaces, transformations, decomposition methods, and structural properties fundamental to data science and linear algebra.

Rather than memorizing definitions, I aimed to internalize how these structures behave geometrically and algebraically.
