# CNN-from-Scratch  
**Numerical Optimization (AI Cyber Security) â€¢ Final Course Project**

*Graduate School Application Portfolio*

**Author**: Seungwoo Lee, Korea University Sejong  
**Course**: Numerical Optimization (Final replacement project)  
**Date**: June 2025  
**Contact**: rainup4632@gmail.com  

---

## ğŸ“˜ Project Context

This work was completed as the **final substitute deliverable** for the Numerical Optimization course. Its purpose is to demonstrateâ€”

1. Deep mathematical understanding of gradient-based learning (deriving backpropagation by hand).  
2. Software craftsmanship by implementing a working CNN **from first principles** (no high-level deep-learning frameworks).  
3. Experimental rigor through controlled evaluation on both a custom small-scale dataset and the MNIST benchmark.

---

## ğŸ”¬ Objectives

- **Implement** every component of a Convolutional Neural Networkâ€”  
  convolutional layers, pooling layers, activation functions (Sigmoid, ReLU, Softmax + cross-entropy), and the SGD optimizerâ€”**using only NumPy**.  
- **Train and evaluate** on:  
  1. A custom 6Ã—6 handwritten-digit dataset (classes â€œ1â€, â€œ2â€, â€œ3â€; 96 samples)  
  2. The standard MNIST dataset (â‰ˆ 90 % test accuracy)  
- **Prove** the mathematical equivalence between:  
  - â€œFlatten â†’ Fully Connectedâ€ and  
  - â€œ2Ã—2-Conv â†’ 1Ã—1 outputâ€  
- **Analyze** convergence behavior under different optimizers and initialization schemes.

---

## ğŸ“Š Key Results

- **Custom 6Ã—6 dataset**  
  â€“ **100 % training accuracy** achieved with NumPy-only CNN + SGD.  
- **MNIST benchmark**  
  â€“ **â‰ˆ 90 % test accuracy**, confirming portability of the implementation.  
- **Optimizer comparison**  
  â€“ SGD vs. Adam vs. RMSprop: Adam converged fastest with lowest variance.  
- **Xavier initialization**  
  â€“ Preserves activation variance across layers, preventing vanishing/exploding gradients.  
- **Layer-design proof**  
  â€“ Formal derivation showing â€œ2Ã—2-Conv â†’ 1Ã—1â€ produces identical outputs to â€œFlatten â†’ FCâ€ for small feature maps.

---

## ğŸ—‚ Repository Contents

001_cnn-from-scratch/
â”œâ”€â”€ Final Project.pdf # Full written report (theory, code architecture, experiments)
â””â”€â”€ README.md # Project summary for portfolio


---

## âš™ï¸ Technical Highlights

- **Layer implementations** (NumPy only):  
  - **Conv2D**, **MaxPool2D**, **Sigmoid**, **ReLU**, **Softmax + Cross-Entropy**  
- **Optimizers from scratch**:  
  - **SGD**, **RMSprop**, **Adam**  
- **Design proof**:  
  - Equivalence of â€œFlattenâ†’FCâ€ and â€œ2Ã—2-Convâ†’1Ã—1â€  
- **Training protocol**:  
  - Full-batch training, fixed random seeds, rigorous hyperparameter controls

---

## ğŸ”® Future Directions

- Add **data augmentation**, **batch normalization**, and **dropout** to guard against overfitting on small datasets.  
- Scale to larger vision tasks (e.g., CIFAR-10) with performance optimization (Cython/Numba/CuPy).  
- Explore advanced optimizers (Adagrad, Nesterov Momentum) and regularization techniques.

---

## ğŸ” Further Reading

1. **Code Repository** (NumPy-only CNN implementation) â†’  
   [https://github.com/seungwoo-AI/cnn-from-scratch
](https://github.com/seungwoo-AI/cnn-from-scratch)

2. **Project Portfolio Page** (screenshots & high-level summary) â†’  
   [https://github.com/seungwoo-AI/seungwoo-AI.github.io/tree/main/Projects/cnn-from-scratch](https://github.com/seungwoo-AI/seungwoo-AI.github.io/tree/main/Projects/001_cnn-from-scratch)

3. **Full 9-page Project Report (PDF)** (derivations & ablations) â†’  
   [https://github.com/seungwoo-AI/seungwoo-AI.github.io/blob/main/Projects/cnn-from-scratch/Final%20Project_2025.pdf](https://github.com/seungwoo-AI/seungwoo-AI.github.io/blob/main/Projects/001_cnn-from-scratch/Final%20Project.pdf)

---

## ğŸ“œ License

All code in the accompanying GitHub repository is released under the **MIT License**.  
